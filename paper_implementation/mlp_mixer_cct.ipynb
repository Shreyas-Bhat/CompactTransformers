{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_mixer_cct.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXuO3WIyyfyQ",
        "outputId": "2dfe4153-dfd2-476c-ed5e-001e10068508"
      },
      "source": [
        "!pip install keras-pos-embd\n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-pos-embd\n",
            "  Downloading keras-pos-embd-0.12.0.tar.gz (6.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-pos-embd) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-pos-embd) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-pos-embd) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-pos-embd) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-pos-embd) (3.13)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-pos-embd) (1.5.2)\n",
            "Building wheels for collected packages: keras-pos-embd\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.12.0-py3-none-any.whl size=7470 sha256=dc3a22eef32d07d8ad17eeb7fbb90fb15e9c0493a9c482d5afd570d81aca251b\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/99/fd/dd98f4876c3ebbef7aab0dbfbd37bca41d7db37d3a28b2cb09\n",
            "Successfully built keras-pos-embd\n",
            "Installing collected packages: keras-pos-embd\n",
            "Successfully installed keras-pos-embd-0.12.0\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 28.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6fhyCD_yOsH"
      },
      "source": [
        "#importing libraries\n",
        "from __future__ import print_function\n",
        "from functools import reduce\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import tempfile\n",
        "import keras\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed, concatenate, Layer\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.regularizers import l2\n",
        "from keras_pos_embd import TrigPosEmbedding\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(1337)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6FYVa2Hhrzs"
      },
      "source": [
        "###*Common functions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcqYc5uuglVg",
        "outputId": "d46541a6-a4f0-4b5f-d806-515a0e2a865d"
      },
      "source": [
        "#loading dataset\n",
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(f\"x_Train shape = {x_train.shape}, y_train shape = {y_train.shape}, x_test shape = {x_test.shape}, y_test shape = {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "x_Train shape = (50000, 32, 32, 3), y_train shape = (50000, 10), x_test shape = (10000, 32, 32, 3), y_test shape = (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY1fZhKln__A"
      },
      "source": [
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "image_size = 64  # We'll resize input images to this size.\n",
        "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2 \n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFOXhKBxgZrE"
      },
      "source": [
        "#function for multi-layer perceptron\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYG9VuCG2i4_"
      },
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, num_patches):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
        "        return patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnACa5jV2uQQ"
      },
      "source": [
        "class MLPMixerLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
        "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.mlp1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=num_patches),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.mlp2 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize(inputs)\n",
        "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "        x_channels = tf.linalg.matrix_transpose(x)\n",
        "        # Apply mlp1 on each channel independently.\n",
        "        mlp1_outputs = self.mlp1(x_channels)\n",
        "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
        "        # Add skip connection.\n",
        "        x = mlp1_outputs + inputs\n",
        "        # Apply layer normalization.\n",
        "        x_patches = self.normalize(x)\n",
        "        # Apply mlp2 on each patch independtenly.\n",
        "        mlp2_outputs = self.mlp2(x_patches)\n",
        "        # Add skip connection.\n",
        "        x = x + mlp2_outputs\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZBilUxIgl7S"
      },
      "source": [
        "# Referred from: github.com:rwightman/pytorch-image-models.\n",
        "class StochasticDepth(layers.Layer):\n",
        "    def __init__(self, drop_prop, **kwargs):\n",
        "        super(StochasticDepth, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prop\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlyhPDb8gs1m"
      },
      "source": [
        "# data augmentation\n",
        "augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.Rescaling(scale=1.0 / 255),\n",
        "        layers.experimental.preprocessing.RandomCrop(image_size, image_size),\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6DhSi_XTJpa"
      },
      "source": [
        "### *Compact Convolutional Transformer (CCT)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcAO4B8K25Gp"
      },
      "source": [
        "#tokenizer consists of convolutional layers\n",
        "#for convolutional tokenization \n",
        "# inputs => embed_to_patches/conv_layer=> linear_projection/pooling => reshape\n",
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 32\n",
        "class Tokenizer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=2,\n",
        "        num_conv_layers=conv_layers,\n",
        "        num_output_channels=[64, 128],\n",
        "        positional_emb=positional_emb,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(Tokenizer, self).__init__(**kwargs)\n",
        "\n",
        "        # convolutional layers\n",
        "        self.conv_model = keras.Sequential()\n",
        "        for i in range(num_conv_layers):\n",
        "            self.conv_model.add(\n",
        "                layers.Conv2D(\n",
        "                    num_output_channels[i],\n",
        "                    kernel_size,\n",
        "                    stride,\n",
        "                    padding=\"valid\",\n",
        "                    use_bias=False,\n",
        "                    activation=\"relu\",\n",
        "                    kernel_initializer=\"he_normal\",\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
        "            #linear pooling \n",
        "            self.conv_model.add(\n",
        "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
        "            )\n",
        "\n",
        "        self.positional_emb = positional_emb\n",
        "        \n",
        "    #reshape \n",
        "    def call(self, images):\n",
        "        outputs = self.conv_model(images)\n",
        "        # After passing the images through our mini-network the spatial dimensions\n",
        "        # are flattened to form sequences.\n",
        "        reshaped = tf.reshape(\n",
        "            outputs,\n",
        "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "        )\n",
        "        return reshaped\n",
        "\n",
        "    #position embedding(optinal)\n",
        "    #calculating the number of sequences and initialize an embedding layer which is learned\n",
        "    def positional_embedding(self, image_size):\n",
        "        # calculating the number of sequences and initialize an embedding layer to\n",
        "        # compute the positional embeddings later\n",
        "        if self.positional_emb:\n",
        "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "            dummy_outputs = self.call(dummy_inputs)\n",
        "            sequence_length = tf.shape(dummy_outputs)[1]\n",
        "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "\n",
        "\n",
        "            embed_layer = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=projection_dim\n",
        "            )\n",
        "            return embed_layer, sequence_length\n",
        "\n",
        "            # embed_layer = keras.models.Sequential()\n",
        "\n",
        "            # embed_layer.add(TrigPosEmbedding(\n",
        "            #     input_dim=sequence_length, \n",
        "            #     output_dim=projection_dim,\n",
        "            #     mode = TrigPosEmbedding.MODE_EXPAND,\n",
        "            # ))\n",
        "            # return embed_layer, sequence_length\n",
        "        else:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TJ4LI2SwUos"
      },
      "source": [
        "#transformer with sequence pooling\n",
        "embedding_dim = 256  # Number of hidden units.\n",
        "def transformer(\n",
        "    image_size = image_size,\n",
        "    input_shape = input_shape,\n",
        "    num_heads = num_heads,\n",
        "    projection_dim = projection_dim,\n",
        "    transformer_units = transformer_units\n",
        "\n",
        "):\n",
        "    inputs = layers.Input(input_shape)\n",
        "    #augmentation\n",
        "    augmented = augmentation(inputs)\n",
        "    #tokenization and encoding patches\n",
        "    tokens = Tokenizer()\n",
        "    encoded_patches = tokens(augmented)\n",
        "\n",
        "    #adding positional embedding\n",
        "    if positional_emb:\n",
        "        embed_layer, seq_len = tokens.positional_embedding(image_size)\n",
        "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "        positional_embeddings = embed_layer(positions)\n",
        "        encoded_patches += positional_embeddings\n",
        "    # Calculate Stochastic Depth probabilities.\n",
        "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
        "\n",
        "    #creating layers from transformer block\n",
        "\n",
        "    for i in range(transformer_layers):\n",
        "        #layer normalization\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "\n",
        "        #create a mult-head attention\n",
        "        attention_output = layers.MultiHeadAttention(num_heads = num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
        "        #skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        #layer normalization 2\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
        "        # MLP.\n",
        "        # x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "\n",
        "        #skip connection 2\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    #apply sequence pooling\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    \n",
        "\n",
        "    # Add MLP.\n",
        "    features = keras.Sequential(\n",
        "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate=0.1) for _ in range(1)]\n",
        ")\n",
        "    #sequence pooling \n",
        "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    weighted_representation = tf.matmul(\n",
        "        attention_weights, representation, transpose_a=True\n",
        "    )\n",
        "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(weighted_representation)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80cM37SxQG2U"
      },
      "source": [
        "#model training \n",
        "\n",
        "def training(model, x_train, y_train):\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
        "    filepath = \"/sample_data/tmp/checkpoint\"\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=True, label_smoothing=0.1\n",
        "        ),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "    es = keras.callbacks.ModelCheckpoint(\n",
        "        filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(x = x_train, y = y_train, batch_size = batch_size, epochs = num_epochs, validation_split= 0.1, callbacks = [es])\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob54kiv3f1d8",
        "outputId": "345b4f61-f8c3-4efb-c901-bcf2e100c760"
      },
      "source": [
        "model = transformer()\n",
        "train = training(model, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "352/352 [==============================] - 370s 1s/step - loss: 1.9122 - accuracy: 0.3405 - top-5-accuracy: 0.8336 - val_loss: 1.6300 - val_accuracy: 0.4740 - val_top-5-accuracy: 0.9248\n",
            "Epoch 2/30\n",
            "352/352 [==============================] - 376s 1s/step - loss: 1.5555 - accuracy: 0.5136 - top-5-accuracy: 0.9369 - val_loss: 1.5133 - val_accuracy: 0.5360 - val_top-5-accuracy: 0.9446\n",
            "Epoch 3/30\n",
            "352/352 [==============================] - 377s 1s/step - loss: 1.4394 - accuracy: 0.5758 - top-5-accuracy: 0.9512 - val_loss: 1.4114 - val_accuracy: 0.5894 - val_top-5-accuracy: 0.9520\n",
            "Epoch 4/30\n",
            "352/352 [==============================] - 370s 1s/step - loss: 1.3645 - accuracy: 0.6140 - top-5-accuracy: 0.9599 - val_loss: 1.3531 - val_accuracy: 0.6124 - val_top-5-accuracy: 0.9604\n",
            "Epoch 5/30\n",
            "352/352 [==============================] - 368s 1s/step - loss: 1.3110 - accuracy: 0.6411 - top-5-accuracy: 0.9644 - val_loss: 1.2897 - val_accuracy: 0.6506 - val_top-5-accuracy: 0.9664\n",
            "Epoch 6/30\n",
            "352/352 [==============================] - 379s 1s/step - loss: 1.2640 - accuracy: 0.6657 - top-5-accuracy: 0.9694 - val_loss: 1.2495 - val_accuracy: 0.6648 - val_top-5-accuracy: 0.9704\n",
            "Epoch 7/30\n",
            "352/352 [==============================] - 374s 1s/step - loss: 1.2313 - accuracy: 0.6820 - top-5-accuracy: 0.9717 - val_loss: 1.2151 - val_accuracy: 0.6862 - val_top-5-accuracy: 0.9730\n",
            "Epoch 8/30\n",
            "352/352 [==============================] - 373s 1s/step - loss: 1.2049 - accuracy: 0.6950 - top-5-accuracy: 0.9732 - val_loss: 1.2165 - val_accuracy: 0.6836 - val_top-5-accuracy: 0.9698\n",
            "Epoch 9/30\n",
            "352/352 [==============================] - 368s 1s/step - loss: 1.1732 - accuracy: 0.7096 - top-5-accuracy: 0.9762 - val_loss: 1.1709 - val_accuracy: 0.7088 - val_top-5-accuracy: 0.9736\n",
            "Epoch 10/30\n",
            "242/352 [===================>..........] - ETA: 1:50 - loss: 1.1466 - accuracy: 0.7229 - top-5-accuracy: 0.9791"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgT0ppMeiVft"
      },
      "source": [
        "#visualizing progress\n",
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uVpNvuQihCF"
      },
      "source": [
        "#visualizing progress\n",
        "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train and Validation Accuracy Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}