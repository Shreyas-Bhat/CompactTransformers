{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compact_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sbo1PzWdTTrF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcW7apsEiGVw",
        "outputId": "0996b78e-1005-4752-e9b5-94e909d85541"
      },
      "source": [
        "!pip install keras-pos-embd\n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-pos-embd\n",
            "  Downloading keras-pos-embd-0.12.0.tar.gz (6.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-pos-embd) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-pos-embd) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-pos-embd) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-pos-embd) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-pos-embd) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-pos-embd) (1.5.2)\n",
            "Building wheels for collected packages: keras-pos-embd\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.12.0-py3-none-any.whl size=7470 sha256=12cba785550461e432daec165bbb96a5dd311150d60adf90e66c2b6c410d0d05\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/99/fd/dd98f4876c3ebbef7aab0dbfbd37bca41d7db37d3a28b2cb09\n",
            "Successfully built keras-pos-embd\n",
            "Installing collected packages: keras-pos-embd\n",
            "Successfully installed keras-pos-embd-0.12.0\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkEw7Oykt0DJ"
      },
      "source": [
        "#importing libraries\n",
        "from __future__ import print_function\n",
        "from functools import reduce\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import tempfile\n",
        "import keras\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed, concatenate, Layer\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.regularizers import l2\n",
        "from keras_pos_embd import TrigPosEmbedding\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(1337)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhF9X3RUhcw7"
      },
      "source": [
        "###*Notes*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fvREZVj0Qa_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "2907c8df-994a-444c-a6bf-d487e21bacb2"
      },
      "source": [
        "'''paper proposes three different modules: ViT lite, Compact Vision Transformers, Compact Convolutional Transformers \n",
        "fewer encoder layers, MHSA heads, and smaller dimensions of patches \n",
        "ViT lite => ViT, with smaller patch sizing, compact version of ViT => sequence pooling => forming compact vision transformer => adding convolutional blocks to the tokenization step to introduce inductive biases\n",
        "\n",
        "CVT => (uses patch-based tokenization)\n",
        "ViT with seqpool => sequential pooling methods, SeqPool, pools sequential based information that results from transformer encoder and eliminates the need for extra classification token\n",
        "\n",
        "CCT => (uses convolitional tokeniztion)\n",
        "convolutional based patching method, which preserves local information and is able to encode relationships between patched, doesnt depend on positional embedding\n",
        "12 layers, 2 convolutional blocks with 7x7 convolutions \n",
        "uses stochastic depth with p = 0.1\n",
        "\n",
        "ViT additional designs\n",
        "class token\n",
        "MLP classifier head on the left\n",
        "positional encoding\n",
        "uses learnable embedding as opposed to sinusoidal embeddings\n",
        "\n",
        "transformers lacks inductive biases \n",
        "\n",
        "convolutional block\n",
        "x0 = max_pool(ReLU(Conv2d(x)))\n",
        "d = 64 filters \n",
        "helps mainting local spatial information\n",
        "\n",
        "transformer based back-bone\n",
        "encoder - transformer block - MSA layer and an MLP head \n",
        "layer norm after positional embeddings\n",
        "GELU activation\n",
        "\n",
        "SeqPool to replace class tokens \n",
        "maps nxd output to nx1 vector \n",
        "sequential outputs => single class \n",
        "assigning important weights across sequence of data\n",
        "pool outputs of the transformed backbone across the sequence \n",
        "T: R(bXnXd) => R(bXd)\n",
        "xL = f(x0)\n",
        "where xL is output of L layer of transformer, b is mini-batch size, n is sequence length, d is embedding dimension\n",
        "=> pass xL to linear layer g(xL) and apply softmax activation:\n",
        "=> xL' = softmax(g(xL).T)\n",
        "=> z = xL'@xL => pooling  = > linear classifier (1xd) output \n",
        "\n",
        "ablation ideas \n",
        "1) study the effects of seqpool, convolutional layers,\n",
        "2) study the effects of learning positional embedding, sinusoidal embedding\n",
        "3) check whether the model needs high dimensions instead of high number of training data\n",
        "4) using involution instead of convolutions \n",
        "5) removing lowest attnetions and using attention rollout https://arxiv.org/pdf/2005.00928.pdf\n",
        "6) using different loss functions (patch wise contrastive loss, patch wise mixing loss) https://arxiv.org/pdf/2104.12753.pdf\n",
        "7) using bio-medical dataset with low train data to test \n",
        "8)study the features learnt by the different models. class activation maps for CNNs and visualize the self-attention maps for transformers.\n",
        "https://arxiv.org/abs/2105.01601\n",
        "https://arxiv.org/abs/2105.08050\n",
        "http://proceedings.mlr.press/v139/tay21a.html\n",
        "https://arxiv.org/abs/2106.10270\n",
        "\n",
        " https://github.com/jacobgil/vit-explain https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples/blob/main/Vision_Transformer_with_tf2.ipynb\n",
        "\n",
        "overview of architecture\n",
        "inputs => convolution => features(reshape) => optional positional embedding => transformer encoder => sequence pooling => mlp head => classification '''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"paper proposes three different modules: ViT lite, Compact Vision Transformers, Compact Convolutional Transformers \\nfewer encoder layers, MHSA heads, and smaller dimensions of patches \\nViT lite => ViT, with smaller patch sizing, compact version of ViT => sequence pooling => forming compact vision transformer => adding convolutional blocks to the tokenization step to introduce inductive biases\\n\\nCVT => (uses patch-based tokenization)\\nViT with seqpool => sequential pooling methods, SeqPool, pools sequential based information that results from transformer encoder and eliminates the need for extra classification token\\n\\nCCT => (uses convolitional tokeniztion)\\nconvolutional based patching method, which preserves local information and is able to encode relationships between patched, doesnt depend on positional embedding\\n12 layers, 2 convolutional blocks with 7x7 convolutions \\nuses stochastic depth with p = 0.1\\n\\nViT additional designs\\nclass token\\nMLP classifier head on the left\\npositional encoding\\nuses learnable embedding as opposed to sinusoidal embeddings\\n\\ntransformers lacks inductive biases \\n\\nconvolutional block\\nx0 = max_pool(ReLU(Conv2d(x)))\\nd = 64 filters \\nhelps mainting local spatial information\\n\\ntransformer based back-bone\\nencoder - transformer block - MSA layer and an MLP head \\nlayer norm after positional embeddings\\nGELU activation\\n\\nSeqPool to replace class tokens \\nmaps nxd output to nx1 vector \\nsequential outputs => single class \\nassigning important weights across sequence of data\\npool outputs of the transformed backbone across the sequence \\nT: R(bXnXd) => R(bXd)\\nxL = f(x0)\\nwhere xL is output of L layer of transformer, b is mini-batch size, n is sequence length, d is embedding dimension\\n=> pass xL to linear layer g(xL) and apply softmax activation:\\n=> xL' = softmax(g(xL).T)\\n=> z = xL'@xL => pooling  = > linear classifier (1xd) output \\n\\nablation ideas \\n1) study the effects of seqpool, convolutional layers,\\n2) study the effects of learning positional embedding, sinusoidal embedding\\n3) check whether the model needs high dimensions instead of high number of training data\\n4) using involution instead of convolutions \\n5) removing lowest attnetions and using attention rollout https://arxiv.org/pdf/2005.00928.pdf\\n6) using different loss functions (patch wise contrastive loss, patch wise mixing loss) https://arxiv.org/pdf/2104.12753.pdf\\n7) using bio-medical dataset with low train data to test \\n8)study the features learnt by the different models. class activation maps for CNNs and visualize the self-attention maps for transformers.\\nhttps://arxiv.org/abs/2105.01601\\nhttps://arxiv.org/abs/2105.08050\\nhttp://proceedings.mlr.press/v139/tay21a.html\\nhttps://arxiv.org/abs/2106.10270\\n\\n https://github.com/jacobgil/vit-explain https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples/blob/main/Vision_Transformer_with_tf2.ipynb\\n\\noverview of architecture\\ninputs => convolution => features(reshape) => optional positional embedding => transformer encoder => sequence pooling => mlp head => classification \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6FYVa2Hhrzs"
      },
      "source": [
        "###*Common functions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcqYc5uuglVg",
        "outputId": "4d1ff7d8-7e48-479f-ae1b-2f5ad5b9de2d"
      },
      "source": [
        "#loading dataset\n",
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "# print(f\"x_Train shape = {x_train.shape}, y_train shape = {y_train.shape}, x_test shape = {x_test.shape}, y_test shape = {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY1fZhKln__A"
      },
      "source": [
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFOXhKBxgZrE"
      },
      "source": [
        "#function for multi-layer perceptron\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZBilUxIgl7S"
      },
      "source": [
        "# Referred from: github.com:rwightman/pytorch-image-models.\n",
        "class StochasticDepth(layers.Layer):\n",
        "    def __init__(self, drop_prop, **kwargs):\n",
        "        super(StochasticDepth, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prop\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlyhPDb8gs1m"
      },
      "source": [
        "# data augmentation\n",
        "augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.Rescaling(scale=1.0 / 255),\n",
        "        layers.experimental.preprocessing.RandomCrop(image_size, image_size),\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyVqPAYebnvt"
      },
      "source": [
        "###*ViT-lite*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHC0HbvZVUzY"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 30\n",
        "image_size = 32 \n",
        "patch_size = 6  \n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64 #projecting patches to this feature size\n",
        "num_heads = 4\n",
        "#paramerization of skip connections \n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JzrS_V1bllu"
      },
      "source": [
        "#implementing patch creation as a layer\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mowjgpp6bqV3"
      },
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        #applying embedding layer on patches\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA6RlMZ-cDhj"
      },
      "source": [
        "#transformer with sequence pooling\n",
        "y = []\n",
        "def transformer(\n",
        "    image_size = image_size,\n",
        "    input_shape = input_shape,\n",
        "    num_heads = num_heads,\n",
        "    projection_dim = projection_dim,\n",
        "    transformer_units = transformer_units\n",
        "):\n",
        "    inputs = layers.Input(input_shape)\n",
        "    #data augmentation\n",
        "    augmented = augmentation(inputs)\n",
        "    #create patches\n",
        "    patches = Patches(patch_size)(inputs)\n",
        "    #enocde patches \n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "    \n",
        "\n",
        "\n",
        "    #adding positional embedding\n",
        "    # if positional_emb:\n",
        "    #     embed_layer, seq_len = Tokenizer.positional_embedding(image_size)\n",
        "    #     positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "    #     positional_embeddings = embed_layer(positions)\n",
        "    #     encoded_patches += positional_embeddings\n",
        "\n",
        "    #creating layers from transformer block\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        y.append(attention_output)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "\n",
        "    #no sequence pooling in ViT lite\n",
        "    \n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=transformer_units, dropout_rate=0.5)\n",
        "    #sequence pooling\n",
        "    # representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    # attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    # weighted_representation = tf.matmul(\n",
        "    #     attention_weights, representation, transpose_a=True\n",
        "    # )\n",
        "    # weighted_representation = tf.squeeze(weighted_representation, -2)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paP8dZV8cGOP"
      },
      "source": [
        "def training(model):\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
        "    filepath = \"/sample_data/tmp/checkpoint\"\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=True, label_smoothing=0.1\n",
        "        ),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            # keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "    es = keras.callbacks.ModelCheckpoint(\n",
        "        filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(x = x_train, y = y_train, batch_size = batch_size, epochs = num_epochs, validation_split= 0.1, callbacks = [es])\n",
        "    accuracy = model.evaluate(x_test, y_test)\n",
        "    # print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    # print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x14gT1jKE42p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f48a2af-a71a-43e0-99fd-3cd63d7ddaab"
      },
      "source": [
        "model = transformer()\n",
        "train = training(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "176/176 [==============================] - 313s 2s/step - loss: 2.3226 - accuracy: 0.1105 - val_loss: 2.2464 - val_accuracy: 0.1684\n",
            "Epoch 2/30\n",
            "176/176 [==============================] - 294s 2s/step - loss: 2.2553 - accuracy: 0.1317 - val_loss: 2.1870 - val_accuracy: 0.1858\n",
            "Epoch 3/30\n",
            "176/176 [==============================] - 298s 2s/step - loss: 2.2328 - accuracy: 0.1375 - val_loss: 2.1504 - val_accuracy: 0.1846\n",
            "Epoch 4/30\n",
            "176/176 [==============================] - 295s 2s/step - loss: 2.2129 - accuracy: 0.1420 - val_loss: 2.1238 - val_accuracy: 0.1958\n",
            "Epoch 5/30\n",
            "176/176 [==============================] - 296s 2s/step - loss: 2.1117 - accuracy: 0.1824 - val_loss: 2.0144 - val_accuracy: 0.2740\n",
            "Epoch 6/30\n",
            "176/176 [==============================] - 296s 2s/step - loss: 2.0046 - accuracy: 0.2503 - val_loss: 1.8912 - val_accuracy: 0.3528\n",
            "Epoch 7/30\n",
            "176/176 [==============================] - 294s 2s/step - loss: 1.9111 - accuracy: 0.3198 - val_loss: 1.7953 - val_accuracy: 0.3932\n",
            "Epoch 8/30\n",
            "176/176 [==============================] - 292s 2s/step - loss: 1.8384 - accuracy: 0.3654 - val_loss: 1.7123 - val_accuracy: 0.4378\n",
            "Epoch 9/30\n",
            "176/176 [==============================] - 296s 2s/step - loss: 1.7723 - accuracy: 0.4016 - val_loss: 1.6998 - val_accuracy: 0.4634\n",
            "Epoch 10/30\n",
            "176/176 [==============================] - 295s 2s/step - loss: 1.7353 - accuracy: 0.4234 - val_loss: 1.6211 - val_accuracy: 0.4808\n",
            "Epoch 11/30\n",
            "176/176 [==============================] - 297s 2s/step - loss: 1.6943 - accuracy: 0.4458 - val_loss: 1.6132 - val_accuracy: 0.4844\n",
            "Epoch 12/30\n",
            "176/176 [==============================] - 302s 2s/step - loss: 1.6705 - accuracy: 0.4605 - val_loss: 1.5808 - val_accuracy: 0.5026\n",
            "Epoch 13/30\n",
            "176/176 [==============================] - 301s 2s/step - loss: 1.6437 - accuracy: 0.4740 - val_loss: 1.5645 - val_accuracy: 0.5118\n",
            "Epoch 14/30\n",
            "176/176 [==============================] - 300s 2s/step - loss: 1.6240 - accuracy: 0.4834 - val_loss: 1.5271 - val_accuracy: 0.5248\n",
            "Epoch 15/30\n",
            "176/176 [==============================] - 297s 2s/step - loss: 1.6006 - accuracy: 0.4957 - val_loss: 1.5234 - val_accuracy: 0.5270\n",
            "Epoch 16/30\n",
            "176/176 [==============================] - 296s 2s/step - loss: 1.5842 - accuracy: 0.5100 - val_loss: 1.4899 - val_accuracy: 0.5394\n",
            "Epoch 17/30\n",
            "176/176 [==============================] - 296s 2s/step - loss: 1.5659 - accuracy: 0.5174 - val_loss: 1.4922 - val_accuracy: 0.5422\n",
            "Epoch 18/30\n",
            "176/176 [==============================] - 300s 2s/step - loss: 1.5486 - accuracy: 0.5259 - val_loss: 1.4699 - val_accuracy: 0.5554\n",
            "Epoch 19/30\n",
            "176/176 [==============================] - 302s 2s/step - loss: 1.5311 - accuracy: 0.5355 - val_loss: 1.4645 - val_accuracy: 0.5598\n",
            "Epoch 20/30\n",
            "176/176 [==============================] - 301s 2s/step - loss: 1.5134 - accuracy: 0.5456 - val_loss: 1.4519 - val_accuracy: 0.5604\n",
            "Epoch 21/30\n",
            "176/176 [==============================] - 308s 2s/step - loss: 1.5037 - accuracy: 0.5487 - val_loss: 1.4358 - val_accuracy: 0.5746\n",
            "Epoch 22/30\n",
            "176/176 [==============================] - 301s 2s/step - loss: 1.4919 - accuracy: 0.5548 - val_loss: 1.4447 - val_accuracy: 0.5634\n",
            "Epoch 23/30\n",
            "176/176 [==============================] - 297s 2s/step - loss: 1.4823 - accuracy: 0.5617 - val_loss: 1.4261 - val_accuracy: 0.5770\n",
            "Epoch 24/30\n",
            "176/176 [==============================] - 298s 2s/step - loss: 1.4641 - accuracy: 0.5699 - val_loss: 1.4458 - val_accuracy: 0.5696\n",
            "Epoch 25/30\n",
            "176/176 [==============================] - 300s 2s/step - loss: 1.4590 - accuracy: 0.5733 - val_loss: 1.4153 - val_accuracy: 0.5804\n",
            "Epoch 26/30\n",
            "176/176 [==============================] - 305s 2s/step - loss: 1.4409 - accuracy: 0.5806 - val_loss: 1.3935 - val_accuracy: 0.5972\n",
            "Epoch 27/30\n",
            "176/176 [==============================] - 298s 2s/step - loss: 1.4413 - accuracy: 0.5814 - val_loss: 1.3947 - val_accuracy: 0.5898\n",
            "Epoch 28/30\n",
            "176/176 [==============================] - 299s 2s/step - loss: 1.4240 - accuracy: 0.5921 - val_loss: 1.3881 - val_accuracy: 0.5946\n",
            "Epoch 29/30\n",
            "176/176 [==============================] - 301s 2s/step - loss: 1.4212 - accuracy: 0.5924 - val_loss: 1.3932 - val_accuracy: 0.5928\n",
            "Epoch 30/30\n",
            "176/176 [==============================] - 299s 2s/step - loss: 1.4093 - accuracy: 0.5971 - val_loss: 1.3855 - val_accuracy: 0.5964\n",
            "313/313 [==============================] - 28s 88ms/step - loss: 1.4020 - accuracy: 0.5903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "fR8-i3r9zb7L",
        "outputId": "4f290809-4b18-4162-9c5e-99f5022d3848"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"original.png\", dpi=300, bbox_inches=\"tight\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")\n",
        "plt.savefig(\"patched.png\", dpi=300, bbox_inches=\"tight\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image size: 32 X 32\n",
            "Patch size: 6 X 6\n",
            "Patches per image: 25\n",
            "Elements per patch: 108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOwAAADnCAYAAAAdFLrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS6ElEQVR4nO3da4xdV3UH8HXOuXfu3Hn7Fcc2tpPg2Imm0GJIkzYPJU1paAWKAhJISBVS1QdSUBTRhMqlVdVCadI2LaUiVQWulD7UmlYqEg0PkxBDHEwjEvyO8/BjsGPPy57xjOdxX+f0Q0i/5P9fd+aWEi/0/31c1/vcPffe5SOtvc/aSVEUJiIxpG/2BERk6ZSwIoEoYUUCUcKKBKKEFQmk5L1Yy3EJOUl5ZTkhLyXk32cJe8UsoaP4Cz9yuZa+283b7Ccyd/4W7JXEnC/KzPK8BYcWScbHsDmQSeS5M+8cT6/a5X/mc4v43fK8xd+r0YTxvXufhvGvfPUf6bXmF8/D+GM7vwrnrTusSCBKWJFAlLAigShhRQJRwooE4laJWcmQVYLl/8+P8yNPCl44ZZXYjBd7X7um4X9QuDMn82BDnHl79W0frlUn3rzJbW54eBjG9x/cQi/19DMn+Pss/a1F5HKkhBUJRAkrEogSViQQJaxIIEpYkUD8ZZ3Ldg/9T6+fyCfurHTkzTqMZ1nFvWRR4M3yWcrvCQXZYM+WiLwPhy8f+cs6KRnHHkx47ZL4mkNDQzD+oQ/+Or3UxMS4905voDusSCBKWJFAlLAigShhRQJRwooE4laJabuXDvZz/xgHXPZYc/bE77Lyo8G0cQodwq+L4zPTvDK5b993YfyuX72HjjEze/wr/w7jv3jLrXTMqtXrYJx0m3EfJCgMt20x66JjXh+5XAX9jrChoVX0tW1b8QMDjO6wIoEoYUUCUcKKBKKEFQlECSsSSEctYvzKGqlYLqVCKsY+28QpzTfrizD+wtGjMP7kk1+j1zr+yssw3rZK/F9fhvGDBw/QMe9+93tg/O1v3w7j3ZUeeq1aq7MqccJaxHTUjoZ8d859cWBghfM+b6Q7rEggSliRQJSwIoEoYUUCUcKKBKKEFQmkoxYx7PxOM+OrOkubTxjeZ7CkTf50LI6fHTlOx3x7zx4YP3jwIIw3Gw16rYHeQfqaZ8XAShifHB2jY770L/8M40f274fx23/pTnqtDVdf7czOQ1rEOF8we7ijIEtEhfGzZleuwJ8bozusSCBKWJFAlLAigShhRQJRwooEkrCKl4hcfnSHFQlECSsSiBJWJBAlrEggSliRQJSwIoG4m/9r9SZc80nJ8Z1mZpbg/wPYhvjM2SjvbKFvt7u+g7Wq5Q7hU5iduQjj/QODbZ8KeOqJr8OJPLNnNx0zPY3fr7u7CuNJyvsc1ev4wYCHP/u37twfvO9eOO9Kmf9YWuQs2ktzszDeNzBAr3XzHb8M4+993wfceY+PnYXz7u/jvZYaLbyZv0Xi3tJpvVaD8SvXrYXz1h1WJBAlrEggSliRQJSwIoEoYUUCadP5n5yDmfPCW5Gy7ufLx2pr7Vqw8PYe/FzPlFyzaOGq6YvHDtNrPfENXNH92Md30DGve/zLuIN+qYt/VT29QzCes+71Kb9WM8dVy3YaOa74ZtbNB5XKMNwziNvULCwu0Es9/p//AePvfd8H+Pub2WNfeBTG77oLn0pgZrZ563UwXqT478nxYouZmWXp8jJDd1iRQJSwIoEoYUUCUcKKBKKEFQlECSsSiLusU5BlnaJw8pwuqbBTBPi16OpNm2Udekivcyjy+OirMP7tb30Lxp9//ll6rcYiXuJYip7ePhhvOX9zkyzfJAneeL9m9Xp6rTNnTjqz4/ICv1de8J9YluFlkEYDH1CdlfiBzn29/sHNzKmTp2D8H3bupGPe8fM3wPgtt+GTCVYO8gcJjhzGpxysWfsrMK47rEggSliRQJSwIoEoYUUCUcKKBOJWifOcb5ZnWDEzZ9v/2QMGZsb2RSep//9MbR63GNn//PfpmCd2fwPGJycnYbynF7dfMTPbfPUWZ3a+odVX4BecvzkhX2PC2vXkvKJaqXR2PuyVV1wF46Uy/4n1VnthfPoibnnTajXptRoN/J230ze0BsabDf5e3/vOd2H8+DF8hu8v3IirymZmZ0ZwVf6WO1UlFglPCSsSiBJWJBAlrEggSliRQNrsJWb7f/me3IRVlkn5mFaPzWxhHrcE6SctRF732M4vwPjRw4fomHIJfxS9Vbx/tVyp0Gtt2YZbiCxFk7Tf8Qr2KWnLw9qPlMt4D6+Z2br1fJ+xZ8XKVTBeOG15Wi38Wm8//n5Z1dvMLE35fl3Pps1bYdz7jddmL8D41BSubj/5xFP0WlX+M4J0hxUJRAkrEogSViQQJaxIIEpYkUCUsCKBuMs6jQbueu8tC7ByeJaxt/LavXRyXoDZ9PQ0jBfO9RoNvMRQb87BeFbnbWDqDXxO6FIskPYyL7/yCh1TIh3016/fAOOVEl5+MDPLss7+Dz8/NQbjrM2QmVlCDhoul/BaR5N8R2ZmNef78ExMTsF4mvFzbRPy9VbJgwRdPbjtj5nZ5Ch+YIDRHVYkECWsSCBKWJFAlLAigShhRQJJvE3OInJ50R1WJBAlrEggSliRQJSwIoEoYUUCUcKKBOJu/j937hxc8+np4ed0sv5C4+MTML5u3VvotdgDA5VK2X0q4J8e+yKcxIHn8VmcZmas9VCTdZt3/qu7g3Rtv/ue97d9muFTf/IZOPfd39xNx7DN8rfddhuMJynfKD82Ngrjjz769+7c77vvXjjvvn6+8b3VxLvo+3oHYLxE/k4zs4kJvIn/r/76IXfeD/zuDjhv1m/KzCxnK6Ep/r2uGOyn15q98EMYf+Rzj8J56w4rEogSViQQJaxIIEpYkUCUsCKBuFXi5557Dsa3b99Ox2SkSrzr3/4Vxn/rdz5GrzU1jVuZbN1yNR1jZpZ24Sr2xmu20THzl/ApAxenz8N4rY5bx5iZZeyQ3CVIclw5Hd7G517uwtXTngo+B3Z2Af+tZmbHjr3ozI47fhyfc3rzzbfSMa0W/lvZObCNxXl6rbGxc87suPFx3Npmfp6/V6OFWyc1c3z/e9f2d9JrLSzUnNm9ke6wIoEoYUUCUcKKBKKEFQlECSsSiBJWJBB3WWfXrl0wvmnTJjrm9Agu7+/ZswfGb7n1TnqtSXJAbrtlnT1798H4xOgkHdOs4yWGnxnGB/7W6uShADMbGRlxZuebGMXLEwN9/IGLgvy/e3EKb4i3snMwcoKXgtpJDJ8+kDd5B/1SCb9XmuLPdnaWn1iwft1aZ3bcmjUrYbzR4Bv2rcDLOg1yGHdKloHMnO+I0B1WJBAlrEggSliRQJSwIoEoYUUCcavEZ0fPwvjJk2fomF27vgTjDVItPHzsJXqt8Um8Mfv9d/8aHWNmduECPh92fp5v2J+dnoHxIrkOxr0jYP/72e/D+L18yP86fx630unr7aZjTp/B39OGDRthfPM119JreRvVPW8bHobxUsYfhMhz0oaFtGDp6e7l1+riLV08qeHfZd7kbXTSFFe3M3IW7uxFXt1eJKsT9L2X9a9F5E2lhBUJRAkrEogSViQQJaxIIG6VuH8IV8NOHH+Fjrn77ntgfH4R7w89OYIbKZuZPfO9vc7suHIJ/1mbnT3QjbW4KpiRiuCa1evptWZKF5zZ+ar9gzC+UFukYy5cnIXxoVV4TJrwM4HXXbHGmR135ZpVMF5zyulsFqx67J1l3Gp2ds4xa8Je6udNy40UvhPyFxVNvu+84TQsR3SHFQlECSsSiBJWJBAlrEggSliRQJSwIoG4yzqW4PYVY+N8KabaXYXxiQt4A/T5Sd625bprebd7z8b1eMllfo53c+8l854jJwLwQ0LNenvw0sxSrFy9AcannPYorQS35Xl1FD88cd21b6XXqjtLPp7FhUswPjvPN9GXusgDDeTkBHZSgJlZK+dLJ56cHAw8N4f/HjOzS+RUgK4ybpPjtffJlzlv3WFFAlHCigSihBUJRAkrEogSViSQxNtQLSKXF91hRQJRwooEooQVCUQJKxKIElYkECWsSCDu5v8P/8a74JpPNdlCx2QJPm9zYvI8jF+zhV+rUsHT+8yf/jFvJ29mf/CHfwTnnXv9c8hm/oVFvOE8S/i5p/V6DcY/9/m/cOdtZvaRj3wUTmTVWt5rad93noLxoX78QMMNP/ez9FrVbrwhf8enPu3O/dOf/H0477kFvvm/3M03xSO1Gv5cPQ8/8rA77wfufwDO+8iRI3TMSyeOw/jGjfikhXe+g3/eR44cgvGv7/4mnLfusCKBKGFFAlHCigSihBUJRAkrEohbJW41cHuUFVf00TGHDpyA8QOHDsN4n9M+4y0bcLuUdvIGrky22HmkZlaQ11jrk4GBAXqtmfO87U07P/jBszB+04030jGb1+MKcoUUshcXSNsbo03t26qTCm5XF++gv7iITyZokk753aSCbWY2Ss4ybufcuVdhvLbI2wmVyMkSpRL+9A6TSrCZ2eI8b0WD6A4rEogSViQQJaxIIEpYkUCUsCKBKGFFAnGXdZp1XKY+ephvjB6fmIPxq67CSzRnfog3UpuZXbqITx5o59D+/TCee/2rSLf5wUG87DQ5iZcDzMzGJzpf1lmzCi8XDVRxV3kzsxtuvxXGkxw/uDBf493m6/WGMzuuRLre5+RzNTObmZmG8emL+JSDri58uLaZ2dGj/DfpeeGFozDOlpbMzFJyWsClGTzvUsY/g7WrVzuzQ+8tImEoYUUCUcKKBKKEFQlECSsSiFsl7q/ic1Y3ruKb8uuNl2F8YBCfmdpbxW1MzMwGelY4s+O2bbkWv+BU61iVc2AAV4n37z9ArzVy6jSfXBszM7MwPnuJbxJ/6Tg+H/YCOXv30hzedG9mNjeHq/wP0BGvefqZfTDunitBKsjlEv4u2HdkZnbV5k3eO1HD118P49Ue/rus9uDfRKULz6/s/O4qzt+E6A4rEogSViQQJaxIIEpYkUCUsCKBuFXi4a03wHjR5Hl+/NQIjJ8+fQrGr9+6lV6rXuNtOjxjpF1IrcEbUeNdt2b1BTxmYZHvud208a30tXaqVbyX+PTZMTqmq4Jbp7BWJt0VXgG9crDDyvz1w+S9eIuYahXPm43x9hKXuzq799x0E/6NpxlvFE9L3wXeY1y0vH3Jy6M7rEggSliRQJSwIoEoYUUCUcKKBKKEFQnEXdaxHJfXT53Em83NzLIEb2ZetQJ3p1+Y58sjNVteV/TXlVL8Z/Wv4CcWVHvxUkd3Gccr3c7m8GqvMzvfe+64HcYTpyd/SjaQZ2RZp+JsRs+yzv4Pf9s2vDzndIihZ/ImZArsdIbXLtXZmQXlnL2ZMyjFL+YtsjjoXCsnS0H0rZf1r0XkTaWEFQlECSsSiBJWJBAlrEggSeE11xaRy4rusCKBKGFFAlHCigSihBUJRAkrEogSViQQd/P/Zx/6M7jm06jX6ZgW2dBdKuEeOWyDuplZQlac7t+xw93p/fmH/xKOTMmmbTOzhLyWkE3l3mZ8I0tlv/3gg213qH/xkUfgYG8TfZ7iFwuy6zzJ+cXYxvvfvP/j7tx3/g2ed97im9vZimKe4x5I3mdQkE38H/3E77nz/rs/fwjOwl3uJJ9RTs7jzZ2HFjLSO+reT3wSzlt3WJFAlLAigShhRQJRwooEooQVCcStEjdzXA1Oy7zwViZV1YxUMhPjFbTEaWXiKRJ8/mnL7dWB55Gly/8/LSF/61Lk6QKMt3LeSocVIdlfmxZOV/tWZw+DLDZn8OWcCmlBVhTyAldbM+f30HQ+H0+tmCJz4PNOWyxtWLWeX6vVWt5vRXdYkUCUsCKBKGFFAlHCigSihBUJRAkrEoi7rDM+dQrGaYdzM+siXffLKV5K8DZZ50453HN28mXyCn+vVgsvC7AN593d+DBiM3+zdzsjoy/AeLPpnJDQIHMnXfwLZ0M+O5S4nZdOPgfj3tXo50Se+mAPBZiZFR3O+8VTeN7e7zIlaZOyJyccxTJPLNAdViQQJaxIIEpYkUCUsCKBKGFFAnGrxIeO7YXxxOnVkZFqcIm0wvA6rXS6hf7YiWeXPYadi8rmkDoPBbDWLEtx4vRhGPcKuzlrBUMfuHA2yiedVVvPTeHKvLf5PyeV2IJs/veq/Ck5l7id0Qun8AteO5ome9qCzM/9jS/vnqk7rEggSliRQJSwIoEoYUUCUcKKBOJWidOuGo57FVJSEauzSqaz/5K1lWmrjNuseO+VkD3QrC65SPbvmv3f9hJfquNWK82mU5knTdpTUvlOM9xCx8ysg444ZmaWZ/gz924JbE86+9q96nundfmkhPcne99hUsav0TFeZ6JlTlx3WJFAlLAigShhRQJRwooEooQVCUQJKxKIv6xDOq0XXsmbLCWw9Z7EqXknHa4x8Dnw69UauLzfIBu9m03eJoed+bkUueGxzYK3R6nX8VzY55Cl/Frlrs4+8zo5MSB1WqBkKd6wT89Z9drAdLiU1iTfr7s0Rzb5s2VDtw2SlnVEfnopYUUCUcKKBKKEFQlECSsSSOJVsETk8qI7rEggSliRQJSwIoEoYUUCUcKKBKKEFQnkfwCg+getsFHSWgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x288 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqvVKaoGi5hK"
      },
      "source": [
        "#visualizing progress\n",
        "plt.plot(train.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(train.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfNm5twRi6Ng"
      },
      "source": [
        "#visualizing progress\n",
        "plt.plot(train.history[\"accuracy\"], label=\"accuracy\")\n",
        "plt.plot(train.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train and Validation Accuracy Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbo1PzWdTTrF"
      },
      "source": [
        "###*Compact Vision Transformer (CVT)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVUNory0Vao_"
      },
      "source": [
        "# positional_emb = True\n",
        "# conv_layers = 2\n",
        "# projection_dim = 128\n",
        "# patch_size = 6\n",
        "# num_heads = 2\n",
        "# transformer_units = [\n",
        "#     projection_dim,\n",
        "#     projection_dim,\n",
        "# ]\n",
        "# transformer_layers = 2\n",
        "# stochastic_depth_rate = 0.1\n",
        "# num_patches = (image_size // patch_size) ** 2\n",
        "# learning_rate = 0.001\n",
        "# weight_decay = 0.0001\n",
        "# batch_size = 128\n",
        "# num_epochs = 30\n",
        "# image_size = 32\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 30\n",
        "image_size = 32 # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "conv_layers = 2\n",
        "positional_emb = True\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWqoi8pbVQ-g"
      },
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQfUe3G_VhJ_"
      },
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OZPSBIOmBzr"
      },
      "source": [
        "#tokenizer consists of convolutional layers\n",
        "#for convolutional tokenization \n",
        "# inputs => embed_to_patches/conv_layer=> linear_projection/pooling => reshape\n",
        "\n",
        "# class Tokenizer(layers.Layer):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         kernel_size=3,\n",
        "#         stride=1,\n",
        "#         padding=1,\n",
        "#         pooling_kernel_size=3,\n",
        "#         pooling_stride=2,\n",
        "#         num_conv_layers=conv_layers,\n",
        "#         num_output_channels=[64, 128],\n",
        "#         positional_emb=positional_emb,\n",
        "#         **kwargs,\n",
        "#     ):\n",
        "#         super(Tokenizer, self).__init__(**kwargs)\n",
        "\n",
        "#         # convolutional layers\n",
        "#         self.conv_model = keras.Sequential()\n",
        "#         for i in range(num_conv_layers):\n",
        "#             self.conv_model.add(\n",
        "#                 layers.Conv2D(\n",
        "#                     num_output_channels[i],\n",
        "#                     kernel_size,\n",
        "#                     stride,\n",
        "#                     padding=\"valid\",\n",
        "#                     use_bias=False,\n",
        "#                     activation=\"relu\",\n",
        "#                     kernel_initializer=\"he_normal\",\n",
        "#                 )\n",
        "#             )\n",
        "            \n",
        "#             self.conv_model.add(layers.ZeroPadding2D(padding))\n",
        "#             #linear pooling \n",
        "#             self.conv_model.add(\n",
        "#                 layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
        "#             )\n",
        "\n",
        "#         self.positional_emb = positional_emb\n",
        "        \n",
        "#     #reshape \n",
        "#     def call(self, images):\n",
        "#         outputs = self.conv_model(images)\n",
        "#         # After passing the images through our mini-network the spatial dimensions\n",
        "#         # are flattened to form sequences.\n",
        "#         reshaped = tf.reshape(\n",
        "#             outputs,\n",
        "#             (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "#         )\n",
        "#         return reshaped\n",
        "\n",
        "#     #position embedding(optinal)\n",
        "#     #calculating the number of sequences and initialize an embedding layer which is learned\n",
        "#     def positional_embedding(self, image_size):\n",
        "#         # Positional embeddings are optional in CCT. Here, we calculate\n",
        "#         # the number of sequences and initialize an `Embedding` layer to\n",
        "#         # compute the positional embeddings later.\n",
        "#         if self.positional_emb:\n",
        "#             dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "#             dummy_outputs = self.call(dummy_inputs)\n",
        "#             sequence_length = tf.shape(dummy_outputs)[1]\n",
        "#             projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "\n",
        "\n",
        "#             embed_layer = layers.Embedding(\n",
        "#                 input_dim=sequence_length, output_dim=projection_dim\n",
        "#             )\n",
        "#             return embed_layer, sequence_length\n",
        "\n",
        "#             # embed_layer = keras.models.Sequential()\n",
        "\n",
        "#             # embed_layer.add(TrigPosEmbedding(\n",
        "#             #     input_dim=sequence_length, \n",
        "#             #     output_dim=projection_dim,\n",
        "#             #     mode = TrigPosEmbedding.MODE_EXPAND,\n",
        "#             # ))\n",
        "#             # return embed_layer, sequence_length\n",
        "#         else:\n",
        "#             return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK7YWIFIUmot"
      },
      "source": [
        "#transformer with sequence pooling\n",
        "\n",
        "def transformer(\n",
        "    image_size = image_size,\n",
        "    input_shape = input_shape,\n",
        "    num_heads = num_heads,\n",
        "    projection_dim = projection_dim,\n",
        "    transformer_units = transformer_units\n",
        "\n",
        "):\n",
        "    inputs = layers.Input(input_shape)\n",
        "    #data augmentation\n",
        "    augmented = augmentation(inputs)\n",
        "    #create patches\n",
        "    patches = Patches(patch_size)(inputs)\n",
        "    #enocde patches \n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "\n",
        "    #adding positional embedding\n",
        "    # if positional_emb:\n",
        "    #     embed_layer, seq_len = Tokenizer.positional_embedding(image_size)\n",
        "    #     positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "    #     positional_embeddings = embed_layer(positions)\n",
        "    #     encoded_patches += positional_embeddings\n",
        "\n",
        "    #creating layers from transformer block\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=transformer_units, dropout_rate=0.5)\n",
        "    #sequence pooling\n",
        "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    weighted_representation = tf.matmul(\n",
        "        attention_weights, representation, transpose_a=True\n",
        "    )\n",
        "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(weighted_representation)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMD64trsUrY1"
      },
      "source": [
        "#model training \n",
        "def training(model):\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
        "    filepath = \"/sample_data/tmp/checkpoint\"\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=True, label_smoothing=0.1\n",
        "        ),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
        "        ],\n",
        "    )\n",
        "    es = keras.callbacks.ModelCheckpoint(\n",
        "        filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(x = x_train, y = y_train, batch_size = batch_size, epochs = num_epochs, validation_split= 0.1, callbacks = [es])\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0_xXgzZU0wR"
      },
      "source": [
        "model = transformer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dunBPSB9J5op",
        "outputId": "742a0463-169b-4ff6-d7b7-1a834f46bf63"
      },
      "source": [
        "train = training(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "352/352 [==============================] - 106s 292ms/step - loss: 2.1122 - accuracy: 0.2273 - val_loss: 1.9935 - val_accuracy: 0.2614\n",
            "Epoch 2/30\n",
            "352/352 [==============================] - 103s 292ms/step - loss: 1.9549 - accuracy: 0.3007 - val_loss: 1.9024 - val_accuracy: 0.3262\n",
            "Epoch 3/30\n",
            "352/352 [==============================] - 104s 294ms/step - loss: 1.8845 - accuracy: 0.3390 - val_loss: 1.8584 - val_accuracy: 0.3454\n",
            "Epoch 4/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.8352 - accuracy: 0.3660 - val_loss: 1.7964 - val_accuracy: 0.3870\n",
            "Epoch 5/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.7934 - accuracy: 0.3920 - val_loss: 1.7833 - val_accuracy: 0.3968\n",
            "Epoch 6/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.7626 - accuracy: 0.4087 - val_loss: 1.7680 - val_accuracy: 0.3994\n",
            "Epoch 7/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.7463 - accuracy: 0.4189 - val_loss: 1.7518 - val_accuracy: 0.4076\n",
            "Epoch 8/30\n",
            "352/352 [==============================] - 104s 295ms/step - loss: 1.7214 - accuracy: 0.4310 - val_loss: 1.6954 - val_accuracy: 0.4438\n",
            "Epoch 9/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.6986 - accuracy: 0.4407 - val_loss: 1.6780 - val_accuracy: 0.4552\n",
            "Epoch 10/30\n",
            "352/352 [==============================] - 103s 292ms/step - loss: 1.6805 - accuracy: 0.4534 - val_loss: 1.6740 - val_accuracy: 0.4420\n",
            "Epoch 11/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.6699 - accuracy: 0.4574 - val_loss: 1.6492 - val_accuracy: 0.4610\n",
            "Epoch 12/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.6586 - accuracy: 0.4647 - val_loss: 1.6677 - val_accuracy: 0.4506\n",
            "Epoch 13/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.6454 - accuracy: 0.4716 - val_loss: 1.6336 - val_accuracy: 0.4704\n",
            "Epoch 14/30\n",
            "352/352 [==============================] - 103s 292ms/step - loss: 1.6332 - accuracy: 0.4768 - val_loss: 1.6478 - val_accuracy: 0.4624\n",
            "Epoch 15/30\n",
            "352/352 [==============================] - 103s 294ms/step - loss: 1.6231 - accuracy: 0.4817 - val_loss: 1.6046 - val_accuracy: 0.4902\n",
            "Epoch 16/30\n",
            "352/352 [==============================] - 104s 294ms/step - loss: 1.6180 - accuracy: 0.4845 - val_loss: 1.6543 - val_accuracy: 0.4624\n",
            "Epoch 17/30\n",
            "352/352 [==============================] - 104s 295ms/step - loss: 1.6013 - accuracy: 0.4936 - val_loss: 1.5984 - val_accuracy: 0.4966\n",
            "Epoch 18/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.5972 - accuracy: 0.4925 - val_loss: 1.5975 - val_accuracy: 0.4844\n",
            "Epoch 19/30\n",
            "352/352 [==============================] - 103s 294ms/step - loss: 1.5867 - accuracy: 0.5004 - val_loss: 1.5881 - val_accuracy: 0.4962\n",
            "Epoch 20/30\n",
            "352/352 [==============================] - 104s 296ms/step - loss: 1.5841 - accuracy: 0.5015 - val_loss: 1.5956 - val_accuracy: 0.4908\n",
            "Epoch 21/30\n",
            "352/352 [==============================] - 104s 295ms/step - loss: 1.5787 - accuracy: 0.5039 - val_loss: 1.5762 - val_accuracy: 0.5028\n",
            "Epoch 22/30\n",
            "352/352 [==============================] - 103s 294ms/step - loss: 1.5657 - accuracy: 0.5070 - val_loss: 1.5952 - val_accuracy: 0.4962\n",
            "Epoch 23/30\n",
            "352/352 [==============================] - 103s 294ms/step - loss: 1.5609 - accuracy: 0.5131 - val_loss: 1.5657 - val_accuracy: 0.5106\n",
            "Epoch 24/30\n",
            "352/352 [==============================] - 103s 292ms/step - loss: 1.5550 - accuracy: 0.5174 - val_loss: 1.5525 - val_accuracy: 0.5170\n",
            "Epoch 25/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.5484 - accuracy: 0.5201 - val_loss: 1.5818 - val_accuracy: 0.4998\n",
            "Epoch 26/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.5406 - accuracy: 0.5214 - val_loss: 1.5768 - val_accuracy: 0.5016\n",
            "Epoch 27/30\n",
            "352/352 [==============================] - 103s 292ms/step - loss: 1.5367 - accuracy: 0.5248 - val_loss: 1.5466 - val_accuracy: 0.5210\n",
            "Epoch 28/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.5325 - accuracy: 0.5271 - val_loss: 1.5365 - val_accuracy: 0.5238\n",
            "Epoch 29/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.5243 - accuracy: 0.5326 - val_loss: 1.5492 - val_accuracy: 0.5260\n",
            "Epoch 30/30\n",
            "352/352 [==============================] - 103s 293ms/step - loss: 1.5202 - accuracy: 0.5336 - val_loss: 1.5233 - val_accuracy: 0.5346\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 1.5394 - accuracy: 0.5246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-cf56a3558431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-b1fe20b42f39>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_5_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test accuracy: {round(accuracy * 100, 2)}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7Ue9sDi2yn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "6cf78ae4-01c0-4e95-af08-1ba306aa4203"
      },
      "source": [
        "#visualizing progress\n",
        "plt.plot(train.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(train.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-cbadd6017b96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#visualizing progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YrI-iQDixy6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "8fdc1204-7cde-4ea1-fc85-3359487e1b77"
      },
      "source": [
        "#visualizing progress\n",
        "plt.plot(train.history[\"accuracy\"], label=\"accuracy\")\n",
        "plt.plot(train.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train and Validation Accuracy Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-96ea9e5db340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#visualizing progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6DhSi_XTJpa"
      },
      "source": [
        "### *Compact Convolutional Transformer (CCT)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcAO4B8K25Gp"
      },
      "source": [
        "#tokenizer consists of convolutional layers\n",
        "#for convolutional tokenization \n",
        "# inputs => embed_to_patches/conv_layer=> linear_projection/pooling => reshape\n",
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 32\n",
        "class Tokenizer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=2,\n",
        "        num_conv_layers=conv_layers,\n",
        "        num_output_channels=[64, 128],\n",
        "        positional_emb=positional_emb,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(Tokenizer, self).__init__(**kwargs)\n",
        "\n",
        "        # convolutional layers\n",
        "        self.conv_model = keras.Sequential()\n",
        "        for i in range(num_conv_layers):\n",
        "            self.conv_model.add(\n",
        "                layers.Conv2D(\n",
        "                    num_output_channels[i],\n",
        "                    kernel_size,\n",
        "                    stride,\n",
        "                    padding=\"valid\",\n",
        "                    use_bias=False,\n",
        "                    activation=\"relu\",\n",
        "                    kernel_initializer=\"he_normal\",\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
        "            #linear pooling \n",
        "            self.conv_model.add(\n",
        "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
        "            )\n",
        "\n",
        "        self.positional_emb = positional_emb\n",
        "        \n",
        "    #reshape \n",
        "    def call(self, images):\n",
        "        outputs = self.conv_model(images)\n",
        "        # After passing the images through our mini-network the spatial dimensions\n",
        "        # are flattened to form sequences.\n",
        "        reshaped = tf.reshape(\n",
        "            outputs,\n",
        "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "        )\n",
        "        return reshaped\n",
        "\n",
        "    #position embedding(optinal)\n",
        "    #calculating the number of sequences and initialize an embedding layer which is learned\n",
        "    def positional_embedding(self, image_size):\n",
        "        # calculating the number of sequences and initialize an embedding layer to\n",
        "        # compute the positional embeddings later\n",
        "        if self.positional_emb:\n",
        "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "            dummy_outputs = self.call(dummy_inputs)\n",
        "            sequence_length = tf.shape(dummy_outputs)[1]\n",
        "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "\n",
        "\n",
        "            embed_layer = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=projection_dim\n",
        "            )\n",
        "            return embed_layer, sequence_length\n",
        "\n",
        "            # embed_layer = keras.models.Sequential()\n",
        "\n",
        "            # embed_layer.add(TrigPosEmbedding(\n",
        "            #     input_dim=sequence_length, \n",
        "            #     output_dim=projection_dim,\n",
        "            #     mode = TrigPosEmbedding.MODE_EXPAND,\n",
        "            # ))\n",
        "            # return embed_layer, sequence_length\n",
        "        else:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TJ4LI2SwUos"
      },
      "source": [
        "#transformer with sequence pooling\n",
        "\n",
        "def transformer(\n",
        "    image_size = image_size,\n",
        "    input_shape = input_shape,\n",
        "    num_heads = num_heads,\n",
        "    projection_dim = projection_dim,\n",
        "    transformer_units = transformer_units\n",
        "\n",
        "):\n",
        "    inputs = layers.Input(input_shape)\n",
        "    #augmentation\n",
        "    augmented = augmentation(inputs)\n",
        "    #tokenization and encoding patches\n",
        "    tokens = Tokenizer()\n",
        "    encoded_patches = tokens(augmented)\n",
        "\n",
        "    #adding positional embedding\n",
        "    if positional_emb:\n",
        "        embed_layer, seq_len = tokens.positional_embedding(image_size)\n",
        "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "        positional_embeddings = embed_layer(positions)\n",
        "        encoded_patches += positional_embeddings\n",
        "    # Calculate Stochastic Depth probabilities.\n",
        "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
        "\n",
        "    #creating layers from transformer block\n",
        "\n",
        "    for i in range(transformer_layers):\n",
        "        #layer normalization\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "\n",
        "        #create a mult-head attention\n",
        "        attention_output = layers.MultiHeadAttention(num_heads = num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
        "        #skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        #layer normalization 2\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "\n",
        "        #skip connection 2\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    #apply sequence pooling\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    \n",
        "\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=transformer_units, dropout_rate=0.5)\n",
        "    #sequence pooling \n",
        "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    weighted_representation = tf.matmul(\n",
        "        attention_weights, representation, transpose_a=True\n",
        "    )\n",
        "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(weighted_representation)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80cM37SxQG2U"
      },
      "source": [
        "#model training \n",
        "\n",
        "def training(model, x_train, y_train):\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
        "    filepath = \"/sample_data/tmp/checkpoint\"\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=True, label_smoothing=0.1\n",
        "        ),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "    es = keras.callbacks.ModelCheckpoint(\n",
        "        filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(x = x_train, y = y_train, batch_size = batch_size, epochs = num_epochs, validation_split= 0.1, callbacks = [es])\n",
        "\n",
        "\n",
        "    return history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob54kiv3f1d8"
      },
      "source": [
        "model = transformer()\n",
        "train = training(model, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgT0ppMeiVft"
      },
      "source": [
        "#visualizing progress\n",
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uVpNvuQihCF"
      },
      "source": [
        "#visualizing progress\n",
        "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train and Validation Accuracy Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}